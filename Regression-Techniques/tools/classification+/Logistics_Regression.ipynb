{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Binary Classification\n",
        "**Goal**: To output `y`, a boolean (e.g, the binary) value classfication result, given a feature  `x`\n",
        "- $x \\rightarrow y,$ where  $x \\in \\mathbb{R}^{n_x}$ & $y \\in {0,1}$\n",
        "\n",
        "We define a training set with m samples as:\n",
        "\n",
        "$$\n",
        "\\{(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})\\}\n",
        "$$\n",
        "### Breaking The Training Set Down\n",
        "X is an input matrix:\n",
        "\n",
        "$$\n",
        "X = \\begin{bmatrix}\n",
        "\\vdots  &         &\\vdots \\\\\n",
        "x^{(1)} & \\cdots  & x^{(m)} \\\\\n",
        "\\vdots  &         & \\vdots\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "- $X \\in \\mathbb{R}^{n_x \\times m}$\n",
        "- $X.\\text{shape} = (n_x, m)$\n",
        "\n",
        "Y is a label vector :\n",
        "\n",
        "$$\n",
        "Y = \\begin{bmatrix}\n",
        "y^{(1)} & \\cdots & y^{(m)}\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "- $Y \\in \\mathbb{R}^{1 \\times m}$\n",
        "- $Y.\\text{shape} = (1, m)$\n",
        "\n",
        "**Intuition**: There are `n_x features` per sample, and  `m samples` total. The more samples m we have, the better the model typically performs\n",
        "\n",
        "\n",
        "**Notes**:\n",
        "- You can think n_x as `rows` and m as `columns` if your brain got lazy, where each i in m is a training sample. Each **`y_i.shape = (1,1)`** is corresponding to a **`X_i.shape = (n_x, 1)`** into a training machine\n",
        "\n",
        "## Logistics Regression\n",
        "**Goal**: Given $x$, we want $\\hat{y} = P(y = 1 | x)$\n",
        "- $X \\in \\mathbb{R}^{n_x x 1}$\n",
        "- Parameter accompied with $X$: $w \\in \\mathbb{R}^{n_x x 1}, b \\in \\mathbb{R}^{1 x 1}$\n",
        "\n",
        "**Notes**:\n",
        "- We are talking about 1 sample training set here\n",
        "\n",
        "### Regression Expression\n",
        "<h4><center>Linear Expression</center></h4>\n",
        "\n",
        "$$\\hat{y} = w^T X + b$$\n",
        "**Notice**\n",
        "- $\\hat{y}$ is not binary bounded: $0 \\leq \\hat{y} \\leq 1$\n",
        "- $w^TX$ has a dimension of $(1, n_x) x (n_x, m) = (1, m)$\n",
        "- $b$ has dimension of (1,m)\n",
        "- **Note**: m is 1 in 1 sample training sample set speaking\n",
        "\n",
        "<h4><center>Logistics Expression</center></h4>\n",
        "\n",
        "$$\\hat{y} = \\sigma(w^T X + b);\\ \\sigma(z) = 1/ (1 + e^{-z})$$\n",
        "\n",
        "**Notice**\n",
        "- As $\\hat{y} \\rightarrow \\infty, \\sigma(z) \\rightarrow 1$\n",
        "- As $\\hat{y} \\rightarrow -\\infty, \\sigma(z) \\rightarrow 0$\n",
        "\n",
        "\n",
        "```python\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "def sigma(z):\n",
        "    return 1 / (1 + np.exp(-z))\n",
        "\n",
        "def linear_model(x):\n",
        "    return 2 * x + 1  # linear model: z = wx + b ; dim -> (len(x), 1)\n",
        "\n",
        "x = np.linspace(-10, 10, 100)\n",
        "z = linear_model(x)\n",
        "y = sigma(z)\n",
        "\n",
        "plt.plot(x, y, label='Sigmoid Output')\n",
        "plt.xlabel('x')\n",
        "plt.ylabel('Probability')\n",
        "plt.title('Logistic Regression: Sigmoid Function')\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "```\n",
        "\n",
        "## Cost Function\n",
        "\n",
        "**Goal**: Given a training set, we want to have each $ \\ \\hat{y}^i \\approx y^i$, where $y^i$ is the `test` value  and $\\hat{y}^i$ is model `trained` predicted value\n",
        "\n",
        "Recall, training set is defined as\n",
        "$$\n",
        "\\{(x^{(1)}, y^{(1)}), \\dots, (x^{(m)}, y^{(m)})\\}\n",
        "$$\n",
        "\n",
        "**`Loss function`** for logistics regression:\n",
        "\n",
        "$$\n",
        " L(\\hat{y}, y) = -(y \\log(\\hat{y}) + (1-y) \\log(1- \\hat{y}))\n",
        "$$\n",
        "**Notice**\n",
        "- At $y = 1: - \\log(\\hat{y}) = L$\n",
        "  - Thus, we want $\\hat{y} \\rightarrow 1: L \\rightarrow 0$\n",
        "- At $y = 0: - \\log(1- \\hat{y}) = L$\n",
        "  - Thus, we want $\\hat{y} \\rightarrow 0: L \\rightarrow 0$\n",
        "\n",
        "\n",
        "\n",
        "**`Average Loss Function`** for m-sample logistics regression\n",
        "\n",
        "$$\n",
        " J(w, b) = \\frac{1}{m} \\sum^m_{i = 1} L(\\hat{y}^i, y^i) = -(y \\log(\\hat{y}) + (1-y) \\log(1- \\hat{y}))\n",
        "$$\n",
        "\n",
        "**Notes**:\n",
        "- Your train and test should all go into you machine to perform the MSE caluclation when in practice\n",
        "- J(w,b) is in expression fo w and b, becuase y is in expression fo w and b. Sigmoid activation function simply limits the value into 0 and 1, but it define the loss here*\n",
        "\n",
        "```\n",
        "notations:\n",
        "     1. *: uncertain, may be disproved\n",
        "\n",
        "```"
      ],
      "metadata": {
        "id": "mBgLUu2jAvng"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ6LL_LGPKVZ",
        "outputId": "72052e34-9fb2-4ab9-8233-47b336d851ae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "100\n"
          ]
        }
      ]
    }
  ]
}